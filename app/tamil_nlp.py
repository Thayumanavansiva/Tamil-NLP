# -*- coding: utf-8 -*-
"""Copy of NEW_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CpabdDU8cgWVf6HCpTFbuzv1hwYIUyji

**TAKING NER AND POS ,THEN COMBINING IT INTO A CANDIDATE KEY FOR COSINE  SIMILARITY**
"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stp=stopwords.words('tamil')

import stanza
stanza.download('ta')
nlp = stanza.Pipeline(lang='ta', processors='tokenize,pos,lemma')

tamil_lemmatized_stopwords = [
    "ஆகு", "உள்", "அமை", "பெறு", "விளங்கு", "உருவாகு", "பெயர்", "நேசி", "வலியுறுத்து",
    "முயல்", "பகிர்", "   எழுது", "உள்ளிடு", "கட்டியெழு", "முக்கியம்", "பங்காற்று", "இன்", "மேன்",
    "என", "உண்டாகு", "கொள்", "அளவு", "நடைபெறு", "தொகு", "வாங்கு", "செய்","உம்",'ஆன'
]

tamil_stopwords = [
    "ஒரு", "என்று", "மற்றும்", "இந்த", "இது", "என்ற", "கொண்டு", "என்பது", "பல", "ஆகும்",
    "அல்லது", "அவர்", "நான்", "உள்ள", "அந்த", "இவர்", "என", "முதல்", "என்ன", "இருந்து",
    "சில", "என்", "போன்ற", "வேண்டும்", "வந்து", "இதன்", "அது", "அவன்", "தான்", "பலரும்",
    "என்னும்", "மேலும்", "பின்னர்", "கொண்ட", "இருக்கும்", "தனது", "உள்ளது", "போது", "என்றும்",
    "அதன்", "தன்", "பிறகு", "அவர்கள்", "வரை", "அவள்", "நீ", "ஆகிய", "இருந்தது", "உள்ளன",
    "வந்த", "இருந்த", "மிகவும்", "இங்கு", "மீது", "ஓர்", "இவை", "இந்தக்", "பற்றி", "வரும்",
    "வேறு", "இரு", "இதில்", "போல்", "இப்போது", "அவரது", "மட்டும்", "இந்தப்", "எனும்", "மேல்",
    "பின்", "சேர்ந்த", "ஆகியோர்", "எனக்கு", "இன்னும்", "அந்தப்", "அன்று", "ஒரே", "மிக", "அங்கு",
    "பல்வேறு", "விட்டு", "பெரும்", "அதை", "பற்றிய", "உன்", "அதிக", "அந்தக்", "பேர்", "இதனால்",
    "அவை", "அதே", "ஏன்", "முறை", "யார்", "என்பதை", "எல்லாம்", "மட்டுமே", "இங்கே", "அங்கே",
    "இடம்", "இடத்தில்", "அதில்", "நாம்", "அதற்கு", "எனவே", "பிற", "சிறு", "மற்ற", "விட",
    "எந்த", "எனவும்", "எனப்படும்", "எனினும்", "அடுத்த", "இதனை", "இதை", "கொள்ள", "இந்தத்",
    "இதற்கு", "அதனால்", "தவிர", "போல", "வரையில்", "சற்று", "எனக்","அந்த", "இந்த", "எந்த", "ஒரு", "தான்", "என்று", "என்ன", "இருந்த", "என்னும்", "இதனால்",
    "ஆனால்", "அதனால்", "எப்படி", "ஏன்", "என்று", "ஏனெனில்", "எப்போது", "என்னை", "உங்களை",
    "எதற்கு", "எதுவும்", "எவர்", "யார்", "எங்கே", "எதனால்", "எதற்காக", "இதற்கு", "அதற்கு",
    "இதனால்", "அதனால்", "இதற்கு", "அதற்கு", "இங்கு", "அங்கு", "எங்கு", "தான்", "தான்",
    "இவை", "அவை", "இவள்", "அவள்", "இவர்", "அவர்", "இவர்கள்", "அவர்கள்", "இது", "அது",
    "இதுவே", "அதுவே", "இவை", "அவை", "இதற்கு", "அதற்கு", "எதற்கு", "எதுவும்", "ஏதாவது",
    "எப்போதும்", "எப்போதும்", "எவ்வளவு", "இப்போது", "அப்போது", "எப்போது", "இதுவரை",
    "அதுவரை", "எவரும்", "யாரும்", "எதுவும்", "ஏதாவது", "எப்போதும்", "எவ்வளவு", "இப்போது",
    "அப்போது", "எப்போது", "இதுவரை", "அதுவரை", "எவரும்", "யாரும்", "எதுவும்", "ஏதாவது",
    "எப்போதும்", "எவ்வளவு", "இப்போது", "அப்போது", "எப்போது", "இதுவரை", "அதுவரை", "எவரும்",
    "யாரும்", "எதுவும்", "ஏதாவது", "எப்போதும்", "எவ்வளவு", "இப்போது", "அப்போது", "எப்போது",
    "இதுவரை", "அதுவரை", "எவரும்", "யாரும்", "எதுவும்", "ஏதாவது", "எப்போதும்", "எவ்வளவு",
    "இப்போது", "அப்போது", "எப்போது", "இதுவரை", "அதுவரை", "எவரும்", "யாரும்", "எதுவும்",
    "ஏதாவது", "எப்போதும்", "எவ்வளவு", "இப்போது", "அப்போது", "எப்போது", "இதுவரை", "அதுவரை",
    "எவரும்", "யாரும்", "எதுவும்", "ஏதாவது", "எப்போதும்", "எவ்வளவு", "இப்போது", "அப்போது",
    "எப்போது", "இதுவரை", "அதுவரை", "எவரும்", "யாரும்", "எதுவும்", "ஏதாவது", "எப்போதும்",
    "எவ்வளவு", "இப்போது", "அப்போது", "எப்போது", "இதுவரை", "அதுவரை", "எவரும்", "யாரும்",
    "எதுவும்", "ஏதாவது", "எப்போதும்", "எவ்வளவு", "இப்போது", "அப்போது", "எப்போது", "இதுவரை",
    "அதுவரை", "எவரும்", "யாரும்", "எதுவும்", "ஏதாவது", "எப்போதும்", "எவ்வளவு", "இப்போது",
    "அப்போது", "எப்போது", "இதுவரை", "அதுவரை", "எவரும்", "யாரும்", "எதுவும்", "ஏதாவது",
    "எப்போதும்", "எவ்வளவு", "இப்போது", "அப்போது", "எப்போது", "இதுவரை", "அதுவரை", "எவரும்",
    "யாரும்", "எதுவும்", "ஏதாவது", "எப்போதும்", "எவ்வளவு", "இப்போது","!", "?", ",", ";", ":", "-", "_", "—", "(", ")", "[", "]", "{", "}",
    "@", "#", "$", "%", "^", "&", "*", "+", "=", "/", "\\", "|", "~", "`",
    "'", "\"","<",">","…" ]

stp.extend(tamil_stopwords)
stp.extend(tamil_lemmatized_stopwords)

"""**----> Coreference resolution**"""

import unicodedata
import string
import math
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from indicnlp.tokenize import sentence_tokenize
from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification
from transformers.pipelines import pipeline
import stanza
from sentence_transformers import SentenceTransformer, util
from indicnlp.normalize.indic_normalize import IndicNormalizerFactory
import re

class TamilNLPProcessor:

    def __init__(self, stopwords: set):
        self.stopwords = stopwords

        # Punctuation setup
        tamil_punctuations = "“”’‘।|!,;:?\"'()-–—[]{}<>…`~"
        self.all_punctuations = set(string.punctuation + tamil_punctuations)

        # Load models
        self.ner_model = "E:\project\Models\BERT-Tamil"
        self.ner_tokenizer = AutoTokenizer.from_pretrained(self.ner_model)
        self.ner_classifier = AutoModelForTokenClassification.from_pretrained(self.ner_model)
        self.ner_pipeline = pipeline("ner", model=self.ner_classifier, tokenizer=self.ner_tokenizer, aggregation_strategy="average")

        self.pos_tagger = stanza.Pipeline(lang='ta', processors='tokenize,pos', use_gpu=False)

        self.summarizer_tokenizer = AutoTokenizer.from_pretrained("E:\project\Models\muril-base-cased")
        self.summarizer_model = AutoModel.from_pretrained("E:\project\Models\muril-base-cased")

        self.similarity_model = SentenceTransformer("E:\project\Models\Tamil-sentence-similarity-sbert")

        self.person_pronouns = ["அவன்", "அவனை", "அவள்", "அவளை", "அவர்கள்", "அவரது", "அவர்", "இவர்கள்"]
        self.object_pronouns = ["அது", "அதை", "அதனை", "இது", "இதனை", "அந்த", "அதன்"]

# pre_processingm the paragraph to get cleaned_text


    def combine_tamil_characters(self, word):
        combined = ''
        for char in word:
            if unicodedata.combining(char):
                combined += char
            else:
                if combined:
                    yield combined
                combined = char
        if combined:
            yield combined


    def get_base_form(self, word):
        suffixes = ["னை", "னிடம்", "ளுக்கு", "ங்களுக்கு", "க்கு", "ஐ", "இடம்", "னும்", "க்குப்", "விடம்", "வனிடம்"] # Corrected suffixes
        for suffix in suffixes:
            if word.endswith(suffix):
                return word[:-len(suffix)]
        return word


    def process_text(self, tamil_text: str):
        sentences = sentence_tokenize.sentence_split(tamil_text, lang='ta')
        clean_tokens_per_sentence = []

        for sentence in sentences:
            words = sentence.split()
            clean_words = []
            for word in words:
                recombined_word = ''.join(self.combine_tamil_characters(word))
                if recombined_word and recombined_word not in self.stopwords:
                    clean_words.append(recombined_word)
            clean_tokens_per_sentence.append(clean_words)

        all_clean_tokens = [token for sentence in clean_tokens_per_sentence for token in sentence]
        preprocessed_text = " ".join(all_clean_tokens)
        return preprocessed_text


    def resolve_coreferences(self, preprocessed_text: str):
        proper_nouns = []
        pronouns_detected = []

        doc = self.pos_tagger(preprocessed_text)
        for sent in doc.sentences:
            for word in sent.words:
                if word.pos == "PROPN" and word.text not in proper_nouns:
                    proper_nouns.append(word.text)
                if word.pos == "PRON" and word.text not in pronouns_detected:
                    pronouns_detected.append(word.text)

        for p in pronouns_detected:
            base = self.get_base_form(p)
            # Ensure base form is not empty before checking in lists
            if base and base in self.person_pronouns and p not in self.person_pronouns:
                self.person_pronouns.append(p)
            elif base and base not in self.person_pronouns and p not in self.object_pronouns:
                self.object_pronouns.append(p)

        person_entities = []
        object_entities = []
        sentences = [s.strip() for s in preprocessed_text.strip().split('.') if s.strip()]
        for sentence in sentences:
            entities = self.ner_pipeline(sentence)
            for entity in entities:
                label = entity["entity_group"]
                word = entity["word"]
                if label == "LABEL_1" and word not in person_entities:
                    person_entities.append(word)
                elif label in ["LABEL_3", "LABEL_5"] and word not in object_entities:
                    object_entities.append(word)

        resolved_sentences = []
        person_pointer = 0
        object_pointer = 0

        for sentence in sentences:
            words = sentence.split()
            resolved_words = []

            for word in words:
                replaced = False
                if word in self.person_pronouns and person_entities:
                    resolved_words.append(person_entities[person_pointer])
                    replaced = True
                elif word in self.object_pronouns and object_entities:
                    resolved_words.append(object_entities[object_pointer])
                    replaced = True
                # Prioritize replacing based on detected entities first
                elif word in person_entities:
                    resolved_words.append(word)
                    person_pointer = min(person_pointer + 1, len(person_entities) - 1)
                    replaced = True
                elif word in object_entities:
                    resolved_words.append(word)
                    object_pointer = min(object_pointer + 1, len(object_entities) - 1)
                    replaced = True

                if not replaced:
                    resolved_words.append(word)

            resolved_sentences.append(' '.join(resolved_words))

        final_paragraph = '. '.join(resolved_sentences)
        # Add a period at the end if it's not already there
        if not final_paragraph.endswith('.'):
             final_paragraph += '.'

        return unicodedata.normalize("NFC", final_paragraph)



    def summarize_text(self, tamil_text: str):
        sentences = sentence_tokenize.sentence_split(tamil_text, lang='ta')
        n = len(sentences)
        # Summarize at least 1 sentence if possible
        summary_count = max(1, math.ceil(n)) # Summarize roughly half the sentences, minimum 1

        if n == 0:
            return ""


        def get_embedding(sentence):
            inputs = self.summarizer_tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
            with torch.no_grad():
                outputs = self.summarizer_model(**inputs)
            attention_mask = inputs['attention_mask']
            embeddings = outputs.last_hidden_state
            mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()
            sum_embeddings = torch.sum(embeddings * mask_expanded, 1)
            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
            return (sum_embeddings / sum_mask).squeeze().numpy()

        sentence_embeddings = [get_embedding(sent) for sent in sentences]
        mean_vector = np.mean(sentence_embeddings, axis=0)
        similarities = cosine_similarity([mean_vector], sentence_embeddings)[0]
        top_indices = similarities.argsort()[-summary_count:][::-1]

        summary_sentences = [sentences[i] for i in sorted(top_indices)]
        joined_summary = ' '.join(sentence.strip() for sentence in summary_sentences)
        # Ensure the summary ends with a period
        if not joined_summary.endswith('.'):
             joined_summary += '.'

        return joined_summary



    def filter_duplicates(self, joined_text: str, threshold: float = 0.8): # Increased threshold slightly
        sentences = sentence_tokenize.sentence_split(joined_text, lang='ta')
        if not sentences:
            return ""

        embeddings = self.similarity_model.encode(sentences, convert_to_tensor=True)
        cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)

        unique_indices = []
        # Keep track of sentences that have been included to avoid adding duplicates
        included_sentences = set()

        for i in range(len(sentences)):
            is_duplicate = False
            # Check similarity against already included unique sentences
            for j in unique_indices:
                if cosine_scores[i][j] > threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_indices.append(i)
                included_sentences.add(sentences[i]) # Add the unique sentence to the set

        unique_sentences = [sentences[i] for i in unique_indices]
        unique_text = " ".join(unique_sentences)
        # Ensure the final text ends with a period
        if not unique_text.endswith('.'):
             unique_text += '.'
        return unique_text



    def process(self, raw_text: str):
        preprocessed = self.process_text(raw_text)
        resolved = self.resolve_coreferences(preprocessed)
        summary = self.summarize_text(resolved)
        final_output = self.filter_duplicates(summary)
        return final_output

#for branches of out mind_map

# Normalizing_txt

    def normalize_tamil_text(self,text:  str):
        factory = IndicNormalizerFactory()
        normalizer = factory.get_normalizer("ta")  # "ta" for Tamil
        cleaned_text = normalizer.normalize(text)
        return cleaned_text


#Actual branching

    def branches_map(self, raw_text: str):
        normalized = self.normalize_tamil_text(raw_text)
        doc = nlp(normalized)

        # Extract clean lemmas and filter
        clean_lemmas = []
        for sentence in doc.sentences:
            for word in sentence.words:
                lemma = word.lemma.strip()
                if lemma not in stp:
                    clean_lemmas.append(lemma)

            # formating outputs in spliting

        def split_tamil_sentences(text):
            # Protect abbreviations/initials like டி.ஜே., கி.பி., டி.வி., பி.எஸ்.
            protected = re.findall(r'(?:[அ-ஹஃௐஂஃ][\u0B80-\u0BFF]?\.?)+', text)

            # Temporarily replace them with placeholders
            for i, item in enumerate(protected):
                safe = f"@@PROTECTED{i}@@"
                text = text.replace(item, safe)

            # Now safely split at sentence-ending period followed by space/newline
            split_text = re.split(r'\.\s+|\.\n+|\.$', text)

            # Restore protected parts
            for i, item in enumerate(protected):
                safe = f"@@PROTECTED{i}@@"
                split_text = [s.replace(safe, item) for s in split_text]

            # Return cleaned list
            return [s.strip() for s in split_text if s.strip()]

        # Join to form lemmatized paragraph
        lemmatized_para = " ".join(clean_lemmas)
        lemma_sent = split_tamil_sentences(lemmatized_para)
        pos_sent=[]
        for i in lemma_sent:
            doc = nlp(i)
            pos_word = [(word.text, word.upos) for sent in doc.sentences for word in sent.words]
            pos = [w for w, t in pos_word if t in {"NOUN", "PROPN","NUM","ADJ"} and w not in tamil_stopwords]
            pos_sent.append(" ".join(pos))
        return pos_sent

def process_tamil_text(input_text: str):
    processor = TamilNLPProcessor(stp)
    cleaned_text = processor.process(input_text)          # to feed into LDA (root)
    pos_sentences = processor.branches_map(input_text)     # to create branches
    return cleaned_text, pos_sentences